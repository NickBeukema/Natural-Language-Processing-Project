After creating our analysis program we began analyzing the readability of different fiction novels from different centuries. We quickly saw that as novel publication dates became more recent, they tended to become more readable. To find this, we tested several popular fiction novels from each century, starting with 1600 to modern day. Comparing one novel from each century we have: Don Quixote 1612, Gulliver’s Travels 1726, Pride and Prejudice 1813, To Kill a Mockingbird 1960, and The Hunger Games 2012. Running each of these texts through our analyzer returned the following results:

These results demonstrate that as the date approaches the 1900’s, popular novels significantly decrease in reading level, from College Graduate to Elementary school. We ran several more popular novels from each century [1] through the analyzer, and they all followed this pattern. We came up with several different speculations for why this might be happening. A possibility is that fiction novels have become very accessible to the mainstream public and are now used as a leisure activity. Additionally, marketability is a massive factor in determining what book companies publish and promote. In a consumerist/capitalist society publishers are motivated primarily by sales than by a desire to promote highbrow literature, and the more readable and enjoyable a book is, the more it will sell. This was not the case when books could not be so easily mass produced, and when literacy and leisure times were privileges reserved for a much smaller segment of the population.
We also found that in general the shorter text is, the lower its reading level will be. This of course depends on what sources you are taking the text from. For example, tweets from the twitter accounts of A-list celebrities such as Kim Kardashian, Chrissy Teigen, and Miley Cyrus seem to average a 6th grade reading level. And even when looking at tweets from even people highly respected in society such as Oprah Winfrey, Barack Obama, or Bill Gates the average reading level was 6th grade. However, longer articles by Oprah, Bill Gates and Barack Obama have reading levels that are around a 10-12th grade to college graduate level. A reason why this occurs is that on platforms such as twitter or a facebook, a person is given a short amount of space to convey their message quickly, leaving less room for longer words or more descriptive and instead requiring a more casual, straight-to-the point type of rhetoric.
One of our most interesting finds was how widely difficulty can vary when comparing one class of article from multiple sources. We decided to look closely into how presidential candidates speak to the general public during an election. Our method was to compare the top 5 candidates from both major political parties from the 2016 election, as well as one candidate from the green party and one candidate from the libertarian party, analyzing their first campaign speech given after announcing their run for presidency. We ran all 12 speeches through our program and found that there was a large range of classifications, ranging from 5th grade to 12th grade. Below is a graph showing candidates classifications and ordered from highest score (lowest classification) to lowest score. 


At first we were surprised to see that these candidates were speaking at such a low reading level, however we came up with several potential reasons for this. The first possible explanation for why these scores are so low is that  when we speak, as opposed to writing, we tend to speak more casually and use not very large complex words. Another possible reason for why these speeches are at a fairly low reading level is that as a politician, a key to getting votes is making the public to relate to you and speaking at a college graduate or above level could cause one to come off as arrogant or pompous to those who did not receive higher education. As we dove further into this potential reason, we began to look more closely at the difference between candidates. We can see that although the reading levels of these speeches are fairly low on average, there are vast differences between particular candidates. For instance, Bernie Sanders received a classification of 10-12th grade,  while John Kasich and Donald Trump’s speeches were classified at a 5th grade reading level. Since all candidates are trying to win over the same electorate, we had initially expected that there would not be such a large difference. However, our results led us to speculate that perhaps different candidates were looking to win over different groups of people. By speaking at a 10-12th grade level it seems Bernie Sanders was hoping to speak to a more educated population - college students or adults who had graduated from college. On the other hand, by speaking at a 5th grade level, Donald Trump and John Kasich were likely hoping to win the vote of adults with less formal education - blue collar workers and the like. The reading level data for the top three and bottom three candidates seems to reinforce this notion, as well as highlight differences between the campaign strategies of different political parties. Republican candidates have the three lowest classifications at 5th, 5th and 6th grade, while the Democrats taking the top three spots at 10-12th, 8-9th, and 8-9th grade. This can’t simply be attributed to Democrats being “better” speakers than republicans, as there may be a good strategy behind giving speeches at a 5th grade level. If the majority of the American electorate is not very well educated, a candidate may want to speak at a lower level in order to seem relatable and down-to-earth to those groups of people. Looking now at the middle of, the data it seems that a large number of people from all parties lie in the middle of both extremes, at a 7th-9th grade classification. Oddly enough, the group of candidates speaking at this reading level includes individuals the most centrist candidates (this is arguable, but most would agree that Hillary Clinton and Ted Cruz are more “moderate” than candidates such as Donald Trump and Bernie Sanders) alongside candidates from third parties that are generally considered to be more fringe. In summary, the broad range in reading levels found in these speeches indicates that these candidates and parties supports the hypothesis that these parties are each trying to appeal to a different voter “base” while simultaneously implementing a strategy to win over just enough undecided voters to tip the scales in their favor. It is also worth mentioning that the first-past-the-post electoral system in the United States almost certainly plays a part in incentivizing this strategy. 
After comparing several different types of text, we found that our algorithm is much less effective when applied to social media posts, specifically tweets, which are restricted in length and are meant to be a more casual format. The length of a text can easily cause incorrect classifications because when only given, for example, 140 characters, one does not have the space to use larger synonyms or more complex words. In addition, although it seems we have created a culture that has allowed tweets to become a form of news or a way to relay important information, it still is a more casual environment, which does not require proper grammar or coherent thoughts and overall incentivizes less complex writing.  
	Our core strategy for raising and lowering the difficulty of a text was simple word replacement. In order to increase the difficulty, our algorithm increases the number of syllables per sentence by replacing words with longer synonyms of themselves. In order to decrease the difficulty, the algorithm replaces words with shorter synonyms. To achieve this, we used the nltk (natural language toolkit) package, which offers the function wordnet.synset(word) as a tool for using a word net to return synonyms of any given word.
	Simply replacing words with longer synonyms has some obvious pitfalls as a strategy, but we reasoned that since difficulty is in part determined by syllable count, almost any effective algorithm for modifying difficulty would have to include some form of word replacement.
	It was fairly simple to build an algorithm that, when given a word as a parameter, returns a longer synonym. The difficulty was in fine-tuning the algorithm to deliver contextually-appropriate word replacements. In english, the exact same word can have multiple entirely different meanings depending on the context in which it is used. A computer can easily access a thesaurus and return the synonym with the most syllables, but the task of ascertaining what “sense” the original word was used in is immensely difficult to abstract onto an algorithm. For example, when given the word “cat” to replace, our algorithm would return “computed axial tomography”. This made sense, from the algorithms “perspective” - if “cat” is interpreted as an acronym (as in “cat scan”), then this is the replacement with the greatest amount of syllables. However, if the sentence is “the cat ate the fly” or “Miles Davis is a cool cat”, then this replacement does not make any sense. 
	Thankfully, nltk returns syllables in “synsets” which refer to the same meaning/sense of the original word: one synset for “cat” includes synonyms for cat as in feline, another contains synonyms for “cat” as in a cool dude who plays jazz. Nltk also sorts these synsets according to how common each sense of the word is. For example, the synset for cat as in feline is the first synset for “cat”, because this is the most common sense of the word. By limiting our set of potential replacements for a word to the synonym set of the most common sense of that word, we were able to make our algorithm return replacements that were generally more sensible - it would no longer replace every instance of the word cat with a word referring to CAT scans. However, this meant that our algorithm would only return accurate replacements for words being used in their most common sense. If the text is a medical article on CAT scans, for instance, it makes sense to modify the reading level by replacing the acronym with the entirety of the words, or vice-versa. Our algorithm cannot do this, because it assumes that “cat” always refers to an animal. Still, this is a “better” problem to have, because these are less common cases. 
	
We’ve found that, although calculating the general score was difficult on smaller texts such as tweets, on some in particular we are able to get a reasonable reading. Even better though, they’re quite a bit more palatable to measure results when augmenting the text to higher and lower reading levels, giving the ability to digest the entire meaning of the text in a reasonable amount of time.
Below is one example of a tweet from Bill Gates being augmented to a higher level of reading with our algorithm:


Strangely enough though, the algorithm used is not deterministic; so after running the text through a few times, this was another output of the same text
